{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we'll attack the MedNIST data set presented in the previous notebook using a deep learning library called _fastai_. We'll also study another, more difficult data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** To run this notebook locally you have to have the fastai library installed. See https://docs.fast.ai for instructions if you want to install on your own computer. If you're using Google Colab or Paperspace Gradient, running the notebook will install what's necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a quick check of whether the notebook is currently running on Google Colaboratory, as that makes some difference for the code below.\n",
    "# We'll do this in every notebook of the course.\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    print('The notebook is running on Colab. colab=True.')\n",
    "    colab=True\n",
    "else:\n",
    "    print('The notebook is not running on Colab. colab=False.')\n",
    "    colab=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to True if you're using Paperspace Gradient:\n",
    "gradient=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if colab or gradient:\n",
    "    !pip install -Uqq fastbook\n",
    "    import fastbook\n",
    "    fastbook.setup_book()\n",
    "    from fastbook import *\n",
    "    !pip install fastai-amalgam\n",
    "    !pip install palettable\n",
    "    !pip install matplotlib_venn\n",
    "    from fastai.vision.all import *\n",
    "    NB_DIR = Path.cwd()\n",
    "else:\n",
    "    from fastai.vision.all import *\n",
    "    NB_DIR = Path.cwd()\n",
    "    DATA = NB_DIR    \n",
    "    \n",
    "if colab:\n",
    "    DATA = Path('./gdrive/MyDrive/ColabData')\n",
    "    DATA.mkdir(exist_ok=True)\n",
    "if gradient:\n",
    "    DATA = Path('/storage')\n",
    "    DATA.mkdir(exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil, gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if gradient:\n",
    "    path = untar_data(\"https://www.dropbox.com/s/5wwskxctvcxiuea/MedNIST.tar.gz?dl=1\", archive='MedNIST.tar.gz', dest='/storage')\n",
    "else:\n",
    "    path = untar_data(\"https://www.dropbox.com/s/5wwskxctvcxiuea/MedNIST.tar.gz?dl=1\", archive='MedNIST.tar.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path.ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set up a data loader, more precisely a fastai [`ImageDataLoader`](https://docs.fast.ai/vision.data.html#ImageDataLoaders), setting aside 30% of the images as validation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames = get_image_files(path)\n",
    "def label_func(x): return x.parent.name\n",
    "dls = ImageDataLoaders.from_path_func(path, fnames, label_func, valid_pct=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of training images: {len(dls.train_ds)}')\n",
    "print(f'Number of validation images: {len(dls.valid_ds)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create what fastai calls a _learner_. It's created from the above dataloaders and its neural network has a specific architecture (and is pretrained on a large 1.4 million image set from the ImageNet competition). Here we use a `resnet18`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = cnn_learner(dls, resnet18, pretrained=True, metrics=accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train it for a bit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fine_tune(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially 100% accurate on the validation data after seconds of training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can have a look at some predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.show_results(figsize=(10,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and also the confusion matrix and the images it found most difficult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp = ClassificationInterpretation.from_learner(learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp.plot_confusion_matrix(figsize=(8,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the four images the model misclassified and the ones it was closest to misclassifying, in the order of model confidence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp.plot_top_losses(9, figsize=(12,12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the problem is too simple when we're using such powerful methods as above. Let's try another!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free up memory:\n",
    "learn = None\n",
    "dls = None\n",
    "path=None\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Another example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a quick check of whether the notebook is currently running on Google Colaboratory, as that makes some difference for the code below.\n",
    "# We'll do this in every notebook of the course.\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    print('The notebook is running on Colab. colab=True.')\n",
    "    colab=True\n",
    "else:\n",
    "    print('The notebook is not running on Colab. colab=False.')\n",
    "    colab=False\n",
    "\n",
    "# Set to True if you're using Paperspace Gradient:\n",
    "gradient=False\n",
    "\n",
    "if colab or gradient:\n",
    "    !pip install -Uqq fastbook\n",
    "    import fastbook\n",
    "    fastbook.setup_book()\n",
    "    from fastbook import *\n",
    "    !pip install fastai-amalgam\n",
    "    !pip install palettable\n",
    "    !pip install matplotlib_venn\n",
    "    from fastai.vision.all import *\n",
    "    NB_DIR = Path.cwd()\n",
    "else:\n",
    "    from fastai.vision.all import *\n",
    "    NB_DIR = Path.cwd()\n",
    "    DATA = NB_DIR    \n",
    "    \n",
    "if colab:\n",
    "    DATA = Path('./gdrive/MyDrive/ColabData')\n",
    "    DATA.mkdir(exist_ok=True)\n",
    "if gradient:\n",
    "    DATA = Path('/storage')\n",
    "    DATA.mkdir(exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil, gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need something more challenging to see what we can achieve with such more powerful models and training methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's keep things relatively simple by using one of the data sets collected in the repository _fast.ai Datasets_ (https://course.fast.ai/datasets). You'll find the options using the previous link, or by looking at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([d for d in dir(URLs) if '__' not in d])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the Caltech-UCSD Birds-200-2011 data sets of 200 different bird species, with 11,788 images in total:<br>\n",
    "<img src=\"assets/birds_collage.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.CUB_200_2011)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's figure out what we've downloaded. In particular, where we can find the images and the corresponding image labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(path/'CUB_200_2011').ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = path/'CUB_200_2011'/'images'\n",
    "images.ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the images are stored in 200 separate subfolders whose names are the bird species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(path/'CUB_200_2011'/'images'/'051.Horned_Grebe').ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 60 Horned Grebes. Here's one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = path/'CUB_200_2011'/'images'/'051.Horned_Grebe'/'Horned_Grebe_0069_34990.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = Image.open(fname)\n",
    "show_image(im, figsize=(10,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a dataloader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_sz=300\n",
    "db = DataBlock(blocks=(ImageBlock, CategoryBlock), \n",
    "               get_items=get_image_files,\n",
    "               get_y=parent_label,\n",
    "               splitter=RandomSplitter(seed=42),\n",
    "               item_tfms=Resize(item_sz),\n",
    "               batch_tfms=Normalize.from_stats(*imagenet_stats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you run out of GPU memory, then you can lower the batch size\n",
    "bs=64\n",
    "dls = db.dataloaders(images, bs=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of training images: {len(dls.train_ds)}')\n",
    "print(f'Number of validation images: {len(dls.valid_ds)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are a few images from one batch (batch size is set to 64, these are 6 of them chosen at random)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls.show_batch(max_n=6, figsize=(12,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a learner as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = cnn_learner(dls, resnet18, pretrained=True, metrics=accuracy).to_fp16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lr = learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fine_tune(5, base_lr=lr.valley)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is this a good result?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the state-of-the-art on the same data set from 2014: https://pub.inf-cv.uni-jena.de/pdf/Goering14:NPT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/goering.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/goering_approach.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not too bad for something that could be constructed this easily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later in this notebook and in the course we'll learn several tricks that could be used to improve the results (e.g. progressive resizing, more advanced data augmentation, ensembling, and more)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are a few predictions on validation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.show_results(figsize=(12,12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp = ClassificationInterpretation.from_learner(learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the images the model was most confident on, while also incorrect. I.e. in some sense the worst mistakes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp.plot_top_losses(6, figsize=(16,12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By extracting the gradients belonging to each of the classes in the data set as they flow into the final convolutional layer in the image, one can produce a heatmap that indicates where in the image the model based its predictions on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/gradcam.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the gradCAM implementation from [fastai-amalgam](https://github.com/Synopsis/amalgam). You can have a look [here](https://github.com/Synopsis/amalgam/blob/master/fastai_amalgam/interpret/gradcam.py) for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install matplotlib_venn fastai_amalgam\n",
    "!conda install --yes --prefix {sys.prefix} palettable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the file `some_utils.py` to check what's imported here\n",
    "# to compute the gradcam maps.\n",
    "sys.path.append(\"../\")\n",
    "from some_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "for img_fn in random.choices(dls.valid_ds.items, k=4):\n",
    "    gcam = gradcam(learn, img_fn, show_original=True)\n",
    "    f = plt.figure(figsize=(16,8))\n",
    "    plt.imshow(gcam)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free up memory:\n",
    "learn = None\n",
    "dls = None\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll do some transformations of the data as in the PyTorch notebook. The rotation and flips are examples of **data augmentation**. By randomly changing the images by rotation and left-right horisontal flips while keeping their labels one can in a sense create \"extra\" training data. And also make the trained model more robust for those transformations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: To speed up processing and use less GPU memory (at the cost of accuracy), \n",
    "# one can set the sizes in the Resize methods to something smaller\n",
    "\n",
    "item_sz = 500\n",
    "batch_sz = 400\n",
    "\n",
    "if colab or gradient:\n",
    "    item_sz = 400\n",
    "    batch_sz = 300\n",
    "    \n",
    "\n",
    "item_tfms = Resize(item_sz, method='pad', pad_mode='zeros')\n",
    "\n",
    "batch_tfms = [Resize(batch_sz, method='pad', pad_mode='zeros'), Flip(), Zoom(),\n",
    "              Contrast(), Rotate(max_deg=20), Normalize.from_stats(*imagenet_stats)]\n",
    "\n",
    "\n",
    "db = DataBlock(blocks=(ImageBlock, CategoryBlock), \n",
    "               get_items=get_image_files,\n",
    "               get_y=parent_label,\n",
    "               splitter=RandomSplitter(seed=42),\n",
    "               item_tfms=item_tfms,\n",
    "               batch_tfms=batch_tfms\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs=32\n",
    "dls = db.dataloaders(images, bs=bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take a look at some data augmentation results for a single image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls.show_batch(max_n=6, figsize=(12,8), unique=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Data augmentation is in general a topic worth thinking hard about each time you face a new data set. Certain transformations may be very important to include as data augmentation (to increase raw model performance or make the model more invariant to certain transformations), others should not be included as they can be completely unrealistic or change what the label of the image should be. In practice, designing good data augmentation strategies may require substantial domain knowledge.\n",
    "\n",
    "> We'll have more to say about this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = cnn_learner(dls, resnet18, metrics=accuracy).to_fp16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lr = learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fine_tune(7, base_lr=lr.valley)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We'll have a look at other, more advanced data augmentation techniques later in the course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test-time augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the data augmentations also at test time by producing multiple predictions for each image, one for each of a set of random data augmentations applied to the image. This is called _test-time augmentation_ or TTA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the result without TTA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds_proba, y_true = learn.get_preds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(y_preds_proba, y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the result with TTA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds_proba, y_true = learn.tta()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(y_preds_proba, y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other CNN model architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used an 18 layer ResNet above. Let's try a couple of different model architectures:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NB:** The below experiment will take a _long_ time to run, and require quite a lot of computational resources. You may want to just browse the code and its output, and try the approach on your own data later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'dn169':       densenet169,\n",
    "    'rn50':        resnet50,\n",
    "    'squeezenet':  squeezenet1_0,\n",
    "    'rn34':        resnet34, \n",
    "    'dn121':       densenet121, \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "preds = {} # For storing the model predictions\n",
    "acc = {}   # For storing the model accuracies\n",
    "\n",
    "for m in models.keys():\n",
    "    print(f\"Training the model {m}\\n\")\n",
    "    learn = cnn_learner(dls, models[m], metrics=accuracy).to_fp16()\n",
    "    lr = learn.lr_find(show_plot=False)\n",
    "    learn.fine_tune(epochs, base_lr=lr.valley)\n",
    "    y_preds_probs, y_true = learn.tta()\n",
    "    preds[m] = y_preds_probs\n",
    "    acc[m] = accuracy(y_preds_probs, y_true)\n",
    "    print(f\"Accuracy for model {m} with TTA is {acc[m]}\")\n",
    "    print('#'*40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here are the training outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/other_models_results.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code used to save precomputed results to disk, for reference\n",
    "#\n",
    "def save_obj(obj, name):\n",
    "    with open(f'assets/{name}.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name):\n",
    "    with open(f'assets/{name}.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "    \n",
    "#save_obj(acc, 'acc')\n",
    "#save_obj(preds, 'preds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the results from disk if you didn't run the above training process:\n",
    "if not colab: \n",
    "    acc = load_obj('acc')\n",
    "    preds = load_obj('preds')\n",
    "if colab:\n",
    "    import urllib\n",
    "    with urllib.request.urlopen('https://github.com/alu042/DAT801/raw/master/extra-deep_learning/assets/acc.pkl') as url:\n",
    "        acc = pickle.load(url)\n",
    "    with urllib.request.urlopen('https://github.com/alu042/DAT801/raw/master/extra-deep_learning/assets/preds.pkl') as url:\n",
    "        preds = pickle.load(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the accuracies for the models we tried:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that most of the models have a similar score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.figure(figsize=(8,6))\n",
    "vals = [float(a) for a in acc.values()]\n",
    "sns.barplot(x=list(acc.keys()), y=vals)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the model predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model ensembling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> TODO: Can use different splits of data to obtain different models: boosting. Then we need a test set.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not colab: \n",
    "    acc = load_obj('acc')\n",
    "    preds = load_obj('preds')\n",
    "if colab:\n",
    "    import urllib\n",
    "    with urllib.request.urlopen('https://github.com/alu042/DAT801/raw/master/extra-deep_learning/assets/acc.pkl') as url:\n",
    "        acc = pickle.load(url)\n",
    "    with urllib.request.urlopen('https://github.com/alu042/DAT801/raw/master/extra-deep_learning/assets/preds.pkl') as url:\n",
    "        preds = pickle.load(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have multiple models of similar performance, a simple trick to obtain an even better model is to construct an _ensemble_. \n",
    "\n",
    "For classifiers, a simple strategy is to have each model in the ensemble vote on a class, and then use the majority class as the final output prediction. This is called _hard voting_. \n",
    "\n",
    "Another frequently used simple strategy is to use the average of the models confidence scores, and then use a threshold to obtain the final predictions. In other words, a model that's more confidence than another model for a given instance contributes more to the prediction. This is called _soft voting_. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example of soft voting, using the models trained above, except the SqueezeNet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del preds['squeezenet']\n",
    "del acc['squeezenet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We add all the predictions from the different models together\n",
    "added_probs = list(preds.values())[0]\n",
    "for p in list(preds.values())[1:]:\n",
    "    added_probs += p\n",
    "# then divide by the number of predictions\n",
    "ensembled_probs = added_probs/len(list(preds.values())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc['ensemble'] = accuracy(ensembled_probs, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that all the above results were obtained with models _pretrained_ on the ImageNet challenge data. This makes the comparison with the 2014 state-of-the-art results unfair, as the ImageNet data set didn't exist back then. \n",
    "\n",
    "Therefore, let's try again without pre-training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = cnn_learner(dls, resnet18, metrics=accuracy, pretrained=False).to_fp16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lr = learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(50, lr_max=lr.valley)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precomputed output:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=40% src=\"assets/cub-no-pretrain.png\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DAT801",
   "language": "python",
   "name": "dat801"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
