{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "from pathlib import Path\n",
    "import seaborn as sns \n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The notebook is not running on Colab. colab=False.\n"
     ]
    }
   ],
   "source": [
    "# This is a quick check of whether the notebook is currently running on Google Colaboratory, as that makes some difference for the code below.\n",
    "# We'll do this in every notebook of the course.\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    print('The notebook is running on Colab. colab=True.')\n",
    "    colab=True\n",
    "else:\n",
    "    print('The notebook is not running on Colab. colab=False.')\n",
    "    colab=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_DIR = Path.cwd()\n",
    "DATA = NB_DIR/'data'\n",
    "DATA.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you know, machine learning models typically have a number of _hyperparameters_ that has to be chosen correctly to get higher performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **What is a hyperparameter?** During training the model's _parameters_ are automatically tuned to make the model produce useful outputs. This is typically achieved by using an optimization algorithm (for example gradient descent) to minimize some cost function (for example mean square error or cross-entropy). \n",
    "\n",
    "> However, there are typically other parameters in the model that are not automatically tuned during training. These are the things you pass to the scikit-learn estimators as parameters. For example `RandomForestClassifier(max_depth = 2, n_estimators = 100, ...)` They are called _hyperparameters_. Examples include things like the learning rate, the amount of regularization, the number of layers in a neural network, and much more. Some models have a large number of such parameters which can influence their performance heavily. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How do we select good hyperparameters?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's essentially a learning task: train the model to also obtain good hyperparameter settings. However, it's typically not that easy to formulate the task in a way where machine learning training methods can work (it's for example difficult to create cost functions for this task that can be optimized using gradient descent, since such cost functions wouldn't be differentiable). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **There are some very interesting methods to make powerful models that optimize ML models, but that is beyond our scope in this notebook**. Have a look at <a href=\"https://ai.googleblog.com/2017/05/using-machine-learning-to-explore.html\">AutoML based on reinforcement learning</a> or <a href=\"https://en.wikipedia.org/wiki/Hyperparameter_optimization#Evolutionary_optimization\">evolutionary algorithms</a> if you're curious."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**One approach to hyperparameter selection: simply search!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Searching for good hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two standard ways are \n",
    "1. **brute-force search**: try all parameter combinations within a specified range, and \n",
    "2. **random search**: try out random combinations of parameter setting within a specified range\n",
    "\n",
    "A third way is to be more clever and use the results obtained from previous parameter settings to select a next setting that is expected to be better. This leads to things like \n",
    "3. **bayesian hyperparameter optimization** \n",
    "\n",
    "and also **evolutionary hyperparameter optimization** (not covered here)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One brute force search method often used is **grid search**. In cases where it makes sense to search through a very large space of parameter settings, or cases where each time you try a setting you have to do a lot of compute, it's better to use random search than grid search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get concrete and try these out on some model trained on some data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll look at a data set of housing prices in various districts in California. To make the story clearer, we'll only tune a few hyperparameters of a random forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if colab:\n",
    "    df = pd.read_csv('https://www.dropbox.com/s/quug4svzdyj5k4j/housing_data.csv?dl=1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not colab:\n",
    "    df = pd.read_csv(DATA/'housing_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>median_house_value</th>\n",
       "      <th>ocean_proximity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-122.13</td>\n",
       "      <td>37.67</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1748.0</td>\n",
       "      <td>318.0</td>\n",
       "      <td>914.0</td>\n",
       "      <td>317.0</td>\n",
       "      <td>3.8676</td>\n",
       "      <td>184000.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-120.98</td>\n",
       "      <td>37.65</td>\n",
       "      <td>40.0</td>\n",
       "      <td>422.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>158.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>7.3841</td>\n",
       "      <td>172200.0</td>\n",
       "      <td>INLAND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-118.37</td>\n",
       "      <td>33.87</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1829.0</td>\n",
       "      <td>331.0</td>\n",
       "      <td>891.0</td>\n",
       "      <td>356.0</td>\n",
       "      <td>6.5755</td>\n",
       "      <td>359900.0</td>\n",
       "      <td>&lt;1H OCEAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-117.89</td>\n",
       "      <td>33.90</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1533.0</td>\n",
       "      <td>226.0</td>\n",
       "      <td>693.0</td>\n",
       "      <td>230.0</td>\n",
       "      <td>7.8980</td>\n",
       "      <td>258200.0</td>\n",
       "      <td>&lt;1H OCEAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-122.40</td>\n",
       "      <td>37.76</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1529.0</td>\n",
       "      <td>385.0</td>\n",
       "      <td>1347.0</td>\n",
       "      <td>348.0</td>\n",
       "      <td>2.9312</td>\n",
       "      <td>239100.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "0    -122.13     37.67                40.0       1748.0           318.0   \n",
       "1    -120.98     37.65                40.0        422.0            63.0   \n",
       "2    -118.37     33.87                23.0       1829.0           331.0   \n",
       "3    -117.89     33.90                23.0       1533.0           226.0   \n",
       "4    -122.40     37.76                52.0       1529.0           385.0   \n",
       "\n",
       "   population  households  median_income  median_house_value ocean_proximity  \n",
       "0       914.0       317.0         3.8676            184000.0        NEAR BAY  \n",
       "1       158.0        63.0         7.3841            172200.0          INLAND  \n",
       "2       891.0       356.0         6.5755            359900.0       <1H OCEAN  \n",
       "3       693.0       230.0         7.8980            258200.0       <1H OCEAN  \n",
       "4      1347.0       348.0         2.9312            239100.0        NEAR BAY  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split off some test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop('median_house_value', axis=1), \n",
    "                                                    df.median_house_value, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>ocean_proximity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2611</th>\n",
       "      <td>-121.57</td>\n",
       "      <td>39.78</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2221.0</td>\n",
       "      <td>459.0</td>\n",
       "      <td>952.0</td>\n",
       "      <td>440.0</td>\n",
       "      <td>2.0458</td>\n",
       "      <td>INLAND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7883</th>\n",
       "      <td>-120.27</td>\n",
       "      <td>34.72</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1289.0</td>\n",
       "      <td>277.0</td>\n",
       "      <td>693.0</td>\n",
       "      <td>237.0</td>\n",
       "      <td>3.2569</td>\n",
       "      <td>&lt;1H OCEAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2818</th>\n",
       "      <td>-122.03</td>\n",
       "      <td>37.98</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1209.0</td>\n",
       "      <td>477.0</td>\n",
       "      <td>627.0</td>\n",
       "      <td>482.0</td>\n",
       "      <td>1.3894</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>-117.98</td>\n",
       "      <td>33.77</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2252.0</td>\n",
       "      <td>570.0</td>\n",
       "      <td>1576.0</td>\n",
       "      <td>550.0</td>\n",
       "      <td>3.6333</td>\n",
       "      <td>&lt;1H OCEAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16139</th>\n",
       "      <td>-118.39</td>\n",
       "      <td>33.87</td>\n",
       "      <td>19.0</td>\n",
       "      <td>3303.0</td>\n",
       "      <td>584.0</td>\n",
       "      <td>1329.0</td>\n",
       "      <td>569.0</td>\n",
       "      <td>7.5210</td>\n",
       "      <td>&lt;1H OCEAN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "2611     -121.57     39.78                18.0       2221.0           459.0   \n",
       "7883     -120.27     34.72                14.0       1289.0           277.0   \n",
       "2818     -122.03     37.98                16.0       1209.0           477.0   \n",
       "219      -117.98     33.77                 7.0       2252.0           570.0   \n",
       "16139    -118.39     33.87                19.0       3303.0           584.0   \n",
       "\n",
       "       population  households  median_income ocean_proximity  \n",
       "2611        952.0       440.0         2.0458          INLAND  \n",
       "7883        693.0       237.0         3.2569       <1H OCEAN  \n",
       "2818        627.0       482.0         1.3894        NEAR BAY  \n",
       "219        1576.0       550.0         3.6333       <1H OCEAN  \n",
       "16139      1329.0       569.0         7.5210       <1H OCEAN  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll measure performance using root mean squared error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 12384 entries, 2611 to 15795\n",
      "Data columns (total 9 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   longitude           12384 non-null  float64\n",
      " 1   latitude            12384 non-null  float64\n",
      " 2   housing_median_age  12384 non-null  float64\n",
      " 3   total_rooms         12384 non-null  float64\n",
      " 4   total_bedrooms      12254 non-null  float64\n",
      " 5   population          12384 non-null  float64\n",
      " 6   households          12384 non-null  float64\n",
      " 7   median_income       12384 non-null  float64\n",
      " 8   ocean_proximity     12384 non-null  object \n",
      "dtypes: float64(8), object(1)\n",
      "memory usage: 967.5+ KB\n"
     ]
    }
   ],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Det mangler noen verdier i `total_bedrooms`. Vi imputerer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "features = ['total_bedrooms']\n",
    "imp = SimpleImputer()\n",
    "X_train.loc[:, features] = imp.fit_transform(X_train[features])\n",
    "X_test.loc[:, features] = imp.transform(X_test[features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Featuren `ocean_proximity` er kategorisk, ikke numerisk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1H OCEAN     5518\n",
       "INLAND        3892\n",
       "NEAR OCEAN    1586\n",
       "NEAR BAY      1385\n",
       "ISLAND           3\n",
       "Name: ocean_proximity, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.ocean_proximity.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi må representere denne på et eller annet egnet vis via **feature encoding**. Vi kan f.eks. erstatte de ulike kategoriene med tall fra 0 til 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ord_enc = OrdinalEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['ocean_proximity'] = ord_enc.fit_transform(X_train[['ocean_proximity']])\n",
    "X_test['ocean_proximity'] = ord_enc.fit_transform(X_test[['ocean_proximity']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>ocean_proximity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2611</th>\n",
       "      <td>-121.57</td>\n",
       "      <td>39.78</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2221.0</td>\n",
       "      <td>459.0</td>\n",
       "      <td>952.0</td>\n",
       "      <td>440.0</td>\n",
       "      <td>2.0458</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7883</th>\n",
       "      <td>-120.27</td>\n",
       "      <td>34.72</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1289.0</td>\n",
       "      <td>277.0</td>\n",
       "      <td>693.0</td>\n",
       "      <td>237.0</td>\n",
       "      <td>3.2569</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2818</th>\n",
       "      <td>-122.03</td>\n",
       "      <td>37.98</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1209.0</td>\n",
       "      <td>477.0</td>\n",
       "      <td>627.0</td>\n",
       "      <td>482.0</td>\n",
       "      <td>1.3894</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>-117.98</td>\n",
       "      <td>33.77</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2252.0</td>\n",
       "      <td>570.0</td>\n",
       "      <td>1576.0</td>\n",
       "      <td>550.0</td>\n",
       "      <td>3.6333</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16139</th>\n",
       "      <td>-118.39</td>\n",
       "      <td>33.87</td>\n",
       "      <td>19.0</td>\n",
       "      <td>3303.0</td>\n",
       "      <td>584.0</td>\n",
       "      <td>1329.0</td>\n",
       "      <td>569.0</td>\n",
       "      <td>7.5210</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "2611     -121.57     39.78                18.0       2221.0           459.0   \n",
       "7883     -120.27     34.72                14.0       1289.0           277.0   \n",
       "2818     -122.03     37.98                16.0       1209.0           477.0   \n",
       "219      -117.98     33.77                 7.0       2252.0           570.0   \n",
       "16139    -118.39     33.87                19.0       3303.0           584.0   \n",
       "\n",
       "       population  households  median_income  ocean_proximity  \n",
       "2611        952.0       440.0         2.0458              1.0  \n",
       "7883        693.0       237.0         3.2569              0.0  \n",
       "2818        627.0       482.0         1.3894              3.0  \n",
       "219        1576.0       550.0         3.6333              0.0  \n",
       "16139      1329.0       569.0         7.5210              0.0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature-skalering:** Det er ikke nødvendig å skalere features da vi kun skal bruke random forest nedenfor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our base model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first create a model using all the standard hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(random_state=42, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check what the standard settings are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'ccp_alpha': 0.0,\n",
       " 'criterion': 'mse',\n",
       " 'max_depth': None,\n",
       " 'max_features': 'auto',\n",
       " 'max_leaf_nodes': None,\n",
       " 'max_samples': None,\n",
       " 'min_impurity_decrease': 0.0,\n",
       " 'min_impurity_split': None,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'min_weight_fraction_leaf': 0.0,\n",
       " 'n_estimators': 100,\n",
       " 'n_jobs': -1,\n",
       " 'oob_score': False,\n",
       " 'random_state': 42,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(n_jobs=-1, random_state=42)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict on test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how the model did:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50759.940462405924"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_test, y_pred, squared=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try to improve our baseline model by tweaking hyperparameters:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In grid search we select some paramaters to change, make a set or range of values for each of them, and try every combination. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We specify our grid as a Python dictionary, or a list of dictionaries if we want to be more specific about parameter combinations to try (for example, \"if `n_estimators` is 10, try `max_depth` 2, 3 and 4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a basic grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 50, 100, 150, 200],\n",
    "    'max_depth': [5, 50, None],\n",
    "    'max_features': [2, 3, None]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can search over all the $4*5=20$ different settings, using 3-fold cross validation to check the performance of each one (training the model a total of $20*3 = 60$ times):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_reg = GridSearchCV(estimator=rf, param_grid=param_grid, cv=3, verbose=1, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 45 candidates, totalling 135 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, estimator=RandomForestRegressor(n_jobs=-1, random_state=42),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'max_depth': [5, 50, None],\n",
       "                         'max_features': [2, 3, None],\n",
       "                         'n_estimators': [10, 50, 100, 150, 200]},\n",
       "             verbose=1)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We grab the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_reg = gs_reg.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and check its parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'ccp_alpha': 0.0,\n",
       " 'criterion': 'mse',\n",
       " 'max_depth': 50,\n",
       " 'max_features': 3,\n",
       " 'max_leaf_nodes': None,\n",
       " 'max_samples': None,\n",
       " 'min_impurity_decrease': 0.0,\n",
       " 'min_impurity_split': None,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'min_weight_fraction_leaf': 0.0,\n",
       " 'n_estimators': 200,\n",
       " 'n_jobs': -1,\n",
       " 'oob_score': False,\n",
       " 'random_state': 42,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_reg.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 50, 'max_features': 3, 'n_estimators': 200}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_reg.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it on our test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49600.15730378219"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_best = best_reg.predict(X_test)\n",
    "mean_squared_error(y_test, y_pred_best, squared=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An improvement over the baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try randomly searching through the space of parameters a specified number of iterations. This can be achieved using `RandomizedSearchCV`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use scikit-learn's `RandomizedSearchCV` in a similar way to `GridSearchCV`: we specify the estimator to use and the parameter grid to search through. But we also specify the number of settings to try, and the method searches randomly through the parameter space that many times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#?RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_reg = RandomizedSearchCV(estimator=rf, param_distributions=param_grid, \n",
    "                            n_iter=n_iter, cv=3, verbose=1, n_jobs=-1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3,\n",
       "                   estimator=RandomForestRegressor(n_jobs=-1, random_state=42),\n",
       "                   n_iter=5, n_jobs=-1,\n",
       "                   param_distributions={'max_depth': [5, 50, None],\n",
       "                                        'max_features': [2, 3, None],\n",
       "                                        'n_estimators': [10, 50, 100, 150,\n",
       "                                                         200]},\n",
       "                   random_state=42, verbose=1)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we're only searching through a selection of the settings, this will of course not outperform the brute-force grid search approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(max_features=3, n_estimators=200, n_jobs=-1,\n",
       "                      random_state=42)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs_reg.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49600.15730378219"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_test,rs_reg.predict(X_test), squared=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As our data is small it doesn't cost much time to search through *all* the settings in our above parameter grid. That is, using grid search is okay. However, if the parameter space was chosen to be much larger, our data set was more complicated, or our model took much longer to train, random search would be a better approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Your turn!** Try searching over a larger set of parameters. You can f.ex. include `bootstrap`, `min_samples_leaf`, `min_samples_split`, `max_features`. See if you can find a combination that scores higher than what we achieved above. Hint: Take a look at the documentation of `RandomForestRegressor` to learn more about the hyperparameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More advanced techniques: Bayesian optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of searching through parameter settings one at a time, without paying attention to what's happened in the past, we can use methods for more intelligent selection of settings to try next. This can significantly reduce the number of iterations necessary to find good parameter settings, and can be useful when looking through a large number of options.\n",
    "\n",
    "In Bayesian hyperparameter optimization you start with an objective function that you want to minimize. It could for example be mean squared error. You then want to search through a space of possible hyperparameters in an efficient way, without having to try all possibilities, and without the trials being completely random. \n",
    "\n",
    "The basic idea of is to somehow build a _model_ of the objective function (more precisely a probability distribution for the objective function) and then use the model to select the most promising hyper parameter settings. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several libraries that can achieve this, for example `hyperopt` https://github.com/hyperopt/hyperopt and `skopt` https://scikit-optimize.github.io."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Using ML-techniques to automate the construction of machine learning systems is called **AutoML**, a field of research that has gotten significant attention in recent years (there are also multiple products offering AutoML solutions, e.g. Google's [Cloud AutoML](https://cloud.google.com/automl/))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `hyperopt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hyperopt in /home/alex/anaconda3/envs/dat801/lib/python3.9/site-packages (0.2.5)\r\n",
      "Requirement already satisfied: tqdm in /home/alex/anaconda3/envs/dat801/lib/python3.9/site-packages (from hyperopt) (4.62.0)\r\n",
      "Requirement already satisfied: networkx>=2.2 in /home/alex/anaconda3/envs/dat801/lib/python3.9/site-packages (from hyperopt) (2.6.2)\r\n",
      "Requirement already satisfied: scipy in /home/alex/anaconda3/envs/dat801/lib/python3.9/site-packages (from hyperopt) (1.7.1)\r\n",
      "Requirement already satisfied: numpy in /home/alex/anaconda3/envs/dat801/lib/python3.9/site-packages (from hyperopt) (1.20.3)\r\n",
      "Requirement already satisfied: six in /home/alex/anaconda3/envs/dat801/lib/python3.9/site-packages (from hyperopt) (1.16.0)\r\n",
      "Requirement already satisfied: future in /home/alex/anaconda3/envs/dat801/lib/python3.9/site-packages (from hyperopt) (0.18.2)\r\n",
      "Requirement already satisfied: cloudpickle in /home/alex/anaconda3/envs/dat801/lib/python3.9/site-packages (from hyperopt) (1.6.0)\r\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp, fmin, tpe, Trials, STATUS_OK, space_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function that we want to minimize during the optimization process. In our case we want to minimize the mean absolute error of our random forest model, as measured using cross validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def objective(params):\n",
    "    # Our objective is to find the hyperparameters that gives the lowest mean squared error \n",
    "    # on validation data\n",
    "    \n",
    "    print(f\"Using parameters {params}\")\n",
    "    # Our model\n",
    "    model = RandomForestRegressor(**params, n_jobs=-1, random_state=42)\n",
    "    # The max cross-val score with current paramaters:\n",
    "    score = cross_val_score(model, X_train, y_train, cv=3, scoring=\"neg_mean_squared_error\").mean()\n",
    "    # We want to minimize the mean absolute error loss, not its negative:\n",
    "    loss = -score\n",
    "    return {'loss': loss, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we select a space of parameters over which to search. We set it up similar to the above `param_grid`, using hyperopt to choose random parameter settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_space = {\n",
    "    'n_estimators': hp.choice('n_estimators', [10, 50, 100, 150, 200]),\n",
    "    'max_depth': hp.choice('max_depth', [5, 50, None]),\n",
    "    'max_features': hp.choice('max_features', [2, 3, None])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can manually select a random (stochastic) samples from this space, just to check that it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hyperopt.pyll.stochastic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 5, 'max_features': 3, 'n_estimators': 50}\n"
     ]
    }
   ],
   "source": [
    "print(hyperopt.pyll.stochastic.sample(param_space))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's minimize our objective. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By using the trials object we can check what goes on in each trial:\n",
    "trials = Trials()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using parameters {'max_depth': 50, 'max_features': 2, 'n_estimators': 10}                                                                                                                                           \n",
      "Using parameters {'max_depth': None, 'max_features': 2, 'n_estimators': 150}                                                                                                                                        \n",
      "Using parameters {'max_depth': 5, 'max_features': None, 'n_estimators': 150}                                                                                                                                        \n",
      "Using parameters {'max_depth': None, 'max_features': None, 'n_estimators': 150}                                                                                                                                     \n",
      "Using parameters {'max_depth': 5, 'max_features': None, 'n_estimators': 10}                                                                                                                                         \n",
      "Using parameters {'max_depth': 5, 'max_features': None, 'n_estimators': 10}                                                                                                                                         \n",
      "Using parameters {'max_depth': 5, 'max_features': None, 'n_estimators': 50}                                                                                                                                         \n",
      "Using parameters {'max_depth': None, 'max_features': None, 'n_estimators': 150}                                                                                                                                     \n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:03<00:00,  2.39trial/s, best loss: 2693148594.7184196]\n"
     ]
    }
   ],
   "source": [
    "best = fmin(fn=objective, space=param_space, algo=tpe.suggest, \n",
    "            max_evals=8, trials=trials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The indices for the best parameters are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 2, 'max_features': 0, 'n_estimators': 3}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `space_eval` function to find the corresponding values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': None, 'max_features': 2, 'n_estimators': 150}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "space_eval(param_space, best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and train and evaluate a model using those values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestRegressor(**space_eval(param_space, best), random_state=42, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50956.919295591484"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train)\n",
    "mean_squared_error(y_test,model.predict(X_test), squared=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Your turn!** Make a function that plots the result of each trial. Hint: use the `trials` object. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: the main component making the above hyperopt approach is the algorithm called `tpe.suggest`. TPE stands for _Tree-structured Parzen Estimator_, and it's what's responsible for building the probability model of our objective function. How it works is unfortunately beyond the scope of our course. If you're curious you can have a look at the [paper introducing the method](https://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf) or look for popularized explanations online (f.ex. [this one](https://towardsdatascience.com/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f).)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your turn!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 1:** Try to search over a much larger set of parameters (and perhaps also over other regression models). Take note of the number of iterations and the total time spent to find a good set of parameters for grid search, random search and hyperopt. \n",
    "\n",
    "> **Exercise 2:** Repeat the process for the data sets studied earlier in the course. Feel free to try other models besides random forests. For example the SGDClassifier, or a SVM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DAT801",
   "language": "python",
   "name": "dat801"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
